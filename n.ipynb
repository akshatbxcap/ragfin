{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\worj\\rag\\rag_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.nn.functional import normalize\n",
    "from groq import Groq\n",
    "# Set environment variable to avoid OpenMP runtime error\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_text_splitter import TextSplitter\n",
    "\n",
    "def load_and_split_texts(folder_path, max_characters=2000):\n",
    "    splitter = TextSplitter(max_characters)\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='latin') as f:\n",
    "                text = f.read()\n",
    "            chunks = splitter.chunks(text)\n",
    "            for chunk in chunks:\n",
    "                data.append({'filename': filename, 'chunk': chunk})\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_embeddings(df, model_id, embeddings_file, index_file,cache_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id,cache_dir=cache_dir)\n",
    "    model = AutoModel.from_pretrained(model_id, add_pooling_layer=False,cache_dir=cache_dir)\n",
    "    model.eval()\n",
    "\n",
    "    chunk_tokens = tokenizer(df['chunk'].tolist(), padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    with torch.inference_mode():\n",
    "        chunk_embeddings = model(**chunk_tokens)[0][:, 0]\n",
    "    chunk_embeddings = normalize(chunk_embeddings).numpy().astype(np.float32)\n",
    "    \n",
    "    # Save embeddings\n",
    "    with open(embeddings_file, 'wb') as f:\n",
    "        pickle.dump(chunk_embeddings, f)\n",
    "    \n",
    "    # Create and save FAISS index\n",
    "    d = chunk_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(chunk_embeddings)\n",
    "    faiss.write_index(index, index_file)\n",
    "\n",
    "    return chunk_embeddings, index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_and_index(embeddings_file, index_file):\n",
    "    with open(embeddings_file, 'rb') as f:\n",
    "        chunk_embeddings = pickle.load(f)\n",
    "    index = faiss.read_index(index_file)\n",
    "    return chunk_embeddings, index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, model_id, index, df, cache_dir, k=5):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    model = AutoModel.from_pretrained(model_id, add_pooling_layer=False,cache_dir=cache_dir)\n",
    "    model.eval()\n",
    "    \n",
    "    query_tokens = tokenizer([query], padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    with torch.inference_mode():\n",
    "        query_embedding = model(**query_tokens)[0][:, 0]\n",
    "    query_embedding = normalize(query_embedding).numpy().astype(np.float32)\n",
    "    \n",
    "    D, I = index.search(query_embedding, k)\n",
    "    relevant_chunks = [df.iloc[i] for i in I[0]]\n",
    "    return relevant_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def expand_query(query, client):\n",
    "#     response = client.chat.completions.create(\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": f\"try to create a small, precise and fictional answer for the query that you feel would be closest to structure with the real answer disregarding facts, answer would then be used to perform similarity search with original document to fined real answer./n/n Query: {query}\"\n",
    "#             }\n",
    "#         ],\n",
    "#         model=\"llama3-8b-8192\",\n",
    "#     )\n",
    "#     return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, relevant_chunks, client):\n",
    "    context = \"\\n\".join([f\"From {chunk['filename']}: {chunk['chunk']}\" for chunk in relevant_chunks])\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"On the basis of these isolated contexts {context}\\n\\nAnswer the query: {query} \\n\\nRemember these are from the policy documents of the organization, consolidate the information and generate a single answer, source does not need to be mentioned and limit the response to 100 words and be concise and precise not creative\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'D:\\Python\\worj\\rag\\foldeer'\n",
    "model_id = \"Snowflake/snowflake-arctic-embed-m-v1.5\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embeddings_file = 'embeddings.pkl'\n",
    "index_file = 'faiss_index.bin'\n",
    "cache_dir = r\"D:\\Python\\worj\\rag\\cache_model\"\n",
    "\n",
    "# Load and split texts\n",
    "df = load_and_split_texts(folder_path)\n",
    "df.to_csv('new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00792541,  0.02521495,  0.05078359, ..., -0.02041577,\n",
       "         -0.04319262,  0.03923333],\n",
       "        [ 0.03838221,  0.05746122,  0.04390368, ...,  0.01192289,\n",
       "         -0.04358421,  0.00834466],\n",
       "        [ 0.07281397,  0.04514536,  0.025077  , ...,  0.01029745,\n",
       "         -0.02703535,  0.00187795],\n",
       "        ...,\n",
       "        [ 0.00658551,  0.06338469,  0.06647727, ..., -0.00087577,\n",
       "         -0.06746212,  0.00215646],\n",
       "        [ 0.07232577,  0.07231921,  0.01632305, ...,  0.03392524,\n",
       "         -0.0396976 ,  0.00459635],\n",
       "        [ 0.06511381,  0.02253478,  0.09496327, ...,  0.03786783,\n",
       "         -0.02904679, -0.00059178]], dtype=float32),\n",
       " <faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x0000022367103C00> >)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and save embeddings and index\n",
    "create_and_save_embeddings(df, model_id, embeddings_file, index_file, cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"when can the full hotel allowance be paid to thee employee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From Travel Reimbursment Policy.txt: Day trip the employee will be paid Daily Allowance as per the following table\\n\\n\\nFull hotel allowance can be avail by the employee only when night stay involves outside of employee Head Quarter.\\nHotel expenses indicated are maximum limits including room rent and service charges applicable excluding Goods & Service Taxes. It√¢\\x80\\x99s is the responsibility of the employee to collect the bills and\\n\\nsubmit with the travel expenses. In case any lodging bill lost then the employee has to collect duplicate lodging bill to claim. Hotel bills will be reimbursed only if supported by proper bills.\\nLocal Conveyance (distribution staff )\\n\\n\\nPetrol bills have to be submitted for claiming local expenses, the employee can take share taxi, cab from uber / Ola etc . The employees are encouraged to use metro/ local train of intra-city travel where ever available\\nAll employees using two-wheeler are encouraged to buy helmets and wear helmets while driving the company will provide a onetime helmet allowance of Rs 500.00 against bill.\\nMobile bill reimbursement ( distribution staff )\\n\\n\\nThe mobile connection and data card (if used) will have to be purchased by the employee and all staff are requested to have post-paid connection as bills will be required for claim\\nThe respective reporting manager to verify, approve and send to Accounts Department at Head Office on or before 10th of every month. Enclose air tickets/boarding passes/AC class rail tickets/hotel lodging/telephone bills etc. The Accounts team will have full authority to process the expenses claims as per the policy and any exception will require approval of the CEO & MD. All claims will be processed only on receipt of the physical claim by the accounts team with email approval of the reporting manager.\\nNOTE : International travel is not covered under this policy and will be circulate to the concern top management who are entitled for international travel\\nFrom Access Control.txt: ACCESS CONTROL POLICY\\nFrom Business Continuity Policy.txt: Business Continuity Policy\\nFrom POSH Policy Xtracap.txt: POLICY FOR PREVENTION OF SEXUAL HARASSMENT AT THE WORKPLACE\\nFrom BCP.txt: Monitoring & Alert\\t:\\tYes'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load embeddings and index\n",
    "chunk_embeddings, index = load_embeddings_and_index(embeddings_file, index_file)\n",
    "\n",
    "client = Groq(api_key=\"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\")\n",
    "# expanded_query = expand_query(query, client)\n",
    "relevant_chunks = retrieve_relevant_chunks(query, model_id, index, df, cache_dir=cache_dir)\n",
    "context = \"\\n\".join([f\"From {chunk['filename']}: {chunk['chunk']}\" for chunk in relevant_chunks])\n",
    "context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full hotel allowance can be availed by the employee when the night stay involves being outside of their Head Quarter.\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(query, relevant_chunks, client)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
